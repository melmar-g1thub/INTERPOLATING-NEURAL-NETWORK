{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C_Q3XlrkkAFq",
        "dENyCvbnj4xb"
      ],
      "authorship_tag": "ABX9TyM5HtObGkxuMwi44biTdCUJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LOCALLY TRAINED NN\n",
        "\n",
        "Implementation of a localized training approach for neural network interpolation.\n",
        "\n",
        "Specifically, it trains the model on small triplets of neighboring data points, allowing the learning of the network on a local scale. By focusing on triplets, the function isolates the network's ability to learn smooth mappings between closely related inputs and outputs, avoiding global patterns and emphasizing local generalization."
      ],
      "metadata": {
        "id": "LPNkHmCPxrNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREPARE TRIPLETS AND MODEL"
      ],
      "metadata": {
        "id": "C_Q3XlrkkAFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for performing inverse transform to predictions to get physical values\n",
        "def inv_log_scaled(values, scaler):\n",
        "    real_val = scaler.inverse_transform(np.array(values).reshape(-1, 1)).flatten()\n",
        "    physical_val = 10**real_val\n",
        "    return physical_val\n",
        "\n",
        "# Function to get triplet indices\n",
        "def get_interpolation_indices(center_index, data_length, gap):\n",
        "    idx1 = center_index - gap\n",
        "    idx3 = center_index + gap\n",
        "    if idx1 < 0 or idx3 >= data_length:\n",
        "        return None\n",
        "    return idx1, center_index, idx3\n",
        "\n",
        "# Function to load activation from string\n",
        "def load_activation_fn(activation_str):\n",
        "    activation_map = {\n",
        "        \"ReLU\": nn.ReLU,\n",
        "        \"LeakyReLU\": nn.LeakyReLU,\n",
        "        \"SiLU\": nn.SiLU\n",
        "    }\n",
        "\n",
        "    if \"LeakyReLU\" in activation_str:\n",
        "        return activation_map[\"LeakyReLU\"]\n",
        "    elif \"ReLU\" in activation_str:\n",
        "        return activation_map[\"ReLU\"]\n",
        "    elif \"SiLU\" in activation_str:\n",
        "        return activation_map[\"SiLU\"]\n",
        "    else:\n",
        "        raise ValueError(f\"Activation function '{activation_str}' not recognized.\")"
      ],
      "metadata": {
        "id": "Is8sd9V2lTxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### We upload the model just as in global interpolation but considerinh input_size=4 because we've concate inputs ######\n",
        "top_path = # path to save metrics and plots\n",
        "output_dir  = os.path.join(top_path, \"Triplet\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "config_path =  #path were config.json is saved\n",
        "def load_model_from_config(config_path, in_size, out_size):\n",
        "    with open(config_path, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    hidden_layer_sizes = config.get(\"hidden_layer_sizes\")\n",
        "    dropout = config.get(\"dropout\")\n",
        "    activation_str = config.get(\"activation_fn\")\n",
        "    activation_cls = load_activation_fn(activation_str)\n",
        "\n",
        "    model = Interpolation(in_size, out_size, hidden_layer_sizes, dropout, activation_cls)\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_triplet = 4 # concate x1, x3 = [T1, nb1, T3, nb3]\n",
        "output_triplet = 2 # P, S\n",
        "\n",
        "# Instantiate the model, without  previous weights just hiperparameters !!!\n",
        "triplet_model = load_model_from_config(config_path=config_path, in_size = input_triplet, out_size = output_triplet)\n",
        "model.to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(triplet_model.parameters(), lr)\n",
        "\n",
        "# 5 random index for both methods\n",
        "max_index = len(in_test)\n",
        "indices = random.sample(range(max_index), 5)\n",
        "print(indices)"
      ],
      "metadata": {
        "id": "PAi4h9efkG6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOCALLY TRAIN MODEL"
      ],
      "metadata": {
        "id": "mKGVL1SElvrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gap = 1\n",
        "# Training triplets\n",
        "in_train_triplets = torch.stack([\n",
        "    torch.cat((in_train_tensor[i - gap], in_train_tensor[i + gap]))\n",
        "    for i in range(gap, len(in_train_tensor) - gap)\n",
        "])\n",
        "out_train_targets = out_train_tensor[gap : len(out_train_tensor) - gap]\n",
        "\n",
        "# Validation triplets\n",
        "in_val_triplets = torch.stack([\n",
        "    torch.cat((in_val_tensor[i - gap], in_val_tensor[i + gap]))\n",
        "    for i in range(gap, len(in_val_tensor) - gap)\n",
        "])\n",
        "out_val_targets = out_val_tensor[gap : len(out_val_tensor) - gap]"
      ],
      "metadata": {
        "id": "ulNDtsp_qKXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_triplet_model(model, in_train, out_train, in_val, out_val, criterion, optimizer, device, epochs, save_path):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds_train = model(in_train.to(device))\n",
        "        loss_train = criterion(preds_train, out_train.to(device))\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds_val = model(in_val.to(device))\n",
        "            loss_val = criterion(preds_val, out_val.to(device))\n",
        "\n",
        "        train_losses.append(loss_train.item())\n",
        "        val_losses.append(loss_val.item())\n",
        "        print(f\"Epoch {epoch+1:03d} | Train Loss: {loss_train.item():.5e} | Val Loss: {loss_val.item():.5e}\")\n",
        "\n",
        "        if loss_val.item() < best_val_loss:\n",
        "            best_val_loss = loss_val.item()\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"  Best model saved (val_loss = {best_val_loss:.5e})\")\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "f0GFdcuDqN8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path  = os.path.join(output_dir, \"best_triplet_model.pth\")\n",
        "\n",
        "# Train triplet model and log losses\n",
        "train_losses, val_losses = train_triplet_model(model=triplet_model, In_train=in_train_triplets, out_train=out_train_targets, in_val=in_val_triplets, out_val=out_val_targets,\n",
        "                                               criterion=criterion, optimizer=optimizer, device=device, epochs=150, save_path=save_path)\n",
        "\n",
        "losses_df = pd.DataFrame({\"epoch\": list(range(1, len(train_losses) + 1)), \"train_loss\": train_losses, \"val_loss\": val_losses})\n",
        "losses_csv_path = os.path.join(output_dir, \"triplet_model_losses.csv\")\n",
        "losses_df.to_csv(losses_csv_path, index=False)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses_df[\"epoch\"], losses_df[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
        "plt.plot(losses_df[\"epoch\"], losses_df[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.title(\"TripletNet Training and Validation Loss\")\n",
        "plt.yscale(\"log\")\n",
        "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, \"triplet_model_loss_curve.png\"), dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QqcZIebqmcBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INTERPOLATION"
      ],
      "metadata": {
        "id": "dENyCvbnj4xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #######################################################\n",
        "# LOCAL NN TRIPLET INTERPOLATION\n",
        "# #######################################################\n",
        "def Interpolation_Triplet_test(model, inputs_set, outputs_set, scaler_P, scaler_S, device, indices, save_dir, gap):\n",
        "    model.eval()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    metrics = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for i, center_index in enumerate(indices):\n",
        "        start_time = time.time()\n",
        "        idx1, idx2, idx3 = get_interpolation_indices(center_index, len(inputs_set), gap)\n",
        "        if idx1 is None:\n",
        "            print(f\"Saltando índice {center_index} (gap={gap}) debido a límites.\")\n",
        "            continue\n",
        "\n",
        "        x1, x2, x3 = in_test[idx1], in_test[idx2], in_test[idx3]\n",
        "        y1, y2_true, y3 = out_test[idx1], out_test[idx2], out_test[idx3]\n",
        "\n",
        "        # Concatenate x1 and x3 for input in the Interpolation NN\n",
        "        triplet_input_tensor = torch.cat([torch.FloatTensor(x1).unsqueeze(0), torch.FloatTensor(x3).unsqueeze(0)], dim=1).to(device)\n",
        "\n",
        "        # Predictions\n",
        "        with torch.no_grad():\n",
        "            y2_pred_scaled = model(triplet_input_tensor).cpu().numpy()[0]\n",
        "\n",
        "        # Get physical outputs\n",
        "        P_true = 10 ** np.array([y1[0], y2_true[0], y3[0]])\n",
        "        S_true = 10 ** np.array([y1[1], y2_true[1], y3[1]])\n",
        "        P2_pred = inv_log_scaled(y2_pred_scaled[0], scaler_P)\n",
        "        S2_pred = inv_log_scaled(y2_pred_scaled[1], scaler_S)\n",
        "\n",
        "        # Metrics\n",
        "        mse_P = mean_squared_error([P_true[1]], [P2_pred])\n",
        "        mae_P = mean_absolute_error([P_true[1]], [P2_pred])\n",
        "        mse_S = mean_squared_error([S_true[1]], [S2_pred])\n",
        "        mae_S = mean_absolute_error([S_true[1]], [S2_pred])\n",
        "        metrics.append({\n",
        "            \"method\": \"TripletNet\", \"index\": int(center_index), \"gap\": gap, \"mse_P\": mse_P, \"mae_P\": mae_P, \"mse_S\": mse_S, \"mae_S\": mae_S})\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        for m in metrics:\n",
        "            m[\"elapsed_sec\"] = elapsed\n",
        "\n",
        "\n",
        "        # Plot\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        x_labels = [f'Idx {idx1}', f'Idx {idx2}', f'Idx {idx3}']\n",
        "        x_ticks = np.arange(3)\n",
        "\n",
        "        axs[0].plot(x_ticks, P_true, label='True P', marker='o', color='green')\n",
        "        axs[0].scatter(x_ticks[1], P2_pred, label='Predicted P', marker='x', s=100, color='red')\n",
        "        axs[0].set_title('Pressure (P) [MeV/fm³]', fontsize=14)\n",
        "        axs[0].set_xticks(x_ticks)\n",
        "        axs[0].set_xticklabels(x_labels, fontsize=14)\n",
        "        axs[0].tick_params(axis='y', labelsize=14)\n",
        "        axs[0].grid(True)\n",
        "        axs[0].legend(fontsize=14)\n",
        "        axs[0].ticklabel_format(axis='y', style='sci', scilimits=(-3, 3))\n",
        "\n",
        "        axs[1].plot(x_ticks, S_true, label='True S', marker='o', color='blue')\n",
        "        axs[1].scatter(x_ticks[1], S2_pred, label='Predicted S', marker='x', s=100, color='orange')\n",
        "        axs[1].set_title('Entropy (S) [1/fm³]', fontsize=14)\n",
        "        axs[1].set_xticks(x_ticks)\n",
        "        axs[1].tick_params(axis='y', labelsize=14)\n",
        "        axs[1].set_xticklabels(x_labels, fontsize=14)\n",
        "        axs[1].grid(True)\n",
        "        axs[1].legend(fontsize=14)\n",
        "        axs[1].ticklabel_format(axis='y', style='sci', scilimits=(-3, 3))\n",
        "\n",
        "        plt.suptitle(f\"NN Local Interpolation Test #{i+1} (Index {center_index})  (Gap {gap})\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        filename = f\"TripletNet_triplet_{i+1}_index_{center_index}.png\"\n",
        "        plt.savefig(os.path.join(save_dir, filename), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "FLKUYggLd6Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload top1 model hyperparameteres\n",
        "output_dir  = # path where top1 model is saved\n",
        "best_model_path = os.path.join(output_dir , \"best_triplet_model.pth\")\n",
        "triplet_model.load_state_dict(torch.load(best_model_path))\n",
        "triplet_model.eval()\n",
        "\n",
        "# Define saving directory\n",
        "triplet_nn_save_base_dir = output_dir\n",
        "os.makedirs(triplet_nn_save_base_dir, exist_ok=True)\n",
        "\n",
        "# Interpolation schemes\n",
        "for current_gap in [1, 5]:\n",
        "    print(f\"\\n TripletNet and B-Spline with gap={current_gap}...\")\n",
        "\n",
        "    save_dir_gap = os.path.join(triplet_nn_save_base_dir, f\"gap_{current_gap}\")\n",
        "    os.makedirs(save_dir_gap, exist_ok=True)\n",
        "\n",
        "    # 1. TripletNet\n",
        "    metrics_triplet_nn_gap = Interpolation_Triplet_test(model=triplet_model, inputs_set=in_test, outputs_set=out_test, scaler_P=scaler_P, scaler_S=scaler_S, device=device, indices=indices, save_dir=save_dir_gap, gap=current_gap)\n",
        "    df_nn = pd.DataFrame(metrics_triplet_nn_gap)\n",
        "\n",
        "    # 2. B-Spline\n",
        "    metrics_spline_gap = interpolation_test_bspline(in_train=in_train, out_train=out_train, in_test=in_test, out_test=out_test, indices=indices, save_dir=save_dir_gap, gap=current_gap)\n",
        "    df_spline = pd.DataFrame(metrics_spline_gap)\n",
        "\n",
        "    # 3. Save metrics\n",
        "    df_all = pd.concat([df_nn, df_spline], ignore_index=True)\n",
        "    csv_path = os.path.join(save_dir_gap, f\"metrics_triplet_gap_{current_gap}.csv\")\n",
        "    df_all.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "id": "0v9r8v3M0Jcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}