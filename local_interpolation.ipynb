{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C_Q3XlrkkAFq",
        "dENyCvbnj4xb"
      ],
      "authorship_tag": "ABX9TyN7wqXUyvAvcdmeYABdaQJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melmar-g1thub/INTERPOLATING-NEURAL-NETWORK/blob/main/local_interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of a localized training approach for neural network interpolation.\n",
        "\n",
        "Specifically, it trains the model on small triplets of neighboring data points, allowing the learning of the network on a local scale. By focusing on triplets, the function isolates the network's ability to learn smooth mappings between closely related inputs and outputs, avoiding global patterns and emphasizing local generalization."
      ],
      "metadata": {
        "id": "LPNkHmCPxrNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREPARE TRIPLETS AND TRAIN"
      ],
      "metadata": {
        "id": "C_Q3XlrkkAFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gap = 1\n",
        "# Training triplets\n",
        "in_train_triplets = torch.stack([\n",
        "    torch.cat((in_train_tensor[i - gap], in_train_tensor[i + gap]))\n",
        "    for i in range(gap, len(in_train_tensor) - gap)\n",
        "])\n",
        "out_train_targets = out_train_tensor[gap : len(out_train_tensor) - gap]\n",
        "\n",
        "# Validation triplets\n",
        "in_val_triplets = torch.stack([\n",
        "    torch.cat((in_val_tensor[i - gap], in_val_tensor[i + gap]))\n",
        "    for i in range(gap, len(in_val_tensor) - gap)\n",
        "])\n",
        "out_val_targets = out_val_tensor[gap : len(out_val_tensor) - gap]"
      ],
      "metadata": {
        "id": "ulNDtsp_qKXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some necessary functions for triplet interpolation\n",
        "# Function for performing inverse transform to predictions to get physical values\n",
        "def inv_log_scaled(values, scaler):\n",
        "    real_val = scaler.inverse_transform(np.array(values).reshape(-1, 1)).flatten()\n",
        "    physical_val = 10**real_val\n",
        "    return physical_val\n",
        "\n",
        "# Get triplet indices\n",
        "def get_interpolation_indices(center_index, data_length, gap):\n",
        "    idx1 = center_index - gap\n",
        "    idx3 = center_index + gap\n",
        "    if idx1 < 0 or idx3 >= data_length:\n",
        "        return None\n",
        "    return idx1, center_index, idx3\n",
        "\n",
        "# Function to load activation from string\n",
        "def load_activation_fn(activation_str):\n",
        "    activation_map = {\n",
        "        \"ReLU\": nn.ReLU,\n",
        "        \"LeakyReLU\": nn.LeakyReLU,\n",
        "        \"SiLU\": nn.SiLU\n",
        "    }\n",
        "\n",
        "    if \"LeakyReLU\" in activation_str:\n",
        "        return activation_map[\"LeakyReLU\"]\n",
        "    elif \"ReLU\" in activation_str:\n",
        "        return activation_map[\"ReLU\"]\n",
        "    elif \"SiLU\" in activation_str:\n",
        "        return activation_map[\"SiLU\"]\n",
        "    else:\n",
        "        raise ValueError(f\"Activation function '{activation_str}' not recognized.\")"
      ],
      "metadata": {
        "id": "K_NFbClXsaOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_path = \"/content/drive/My Drive/Colab Notebooks/Random_16_06\"\n",
        "output_dir  = os.path.join(top_path, \"Triplet\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/Random_16_06/top1/config.json\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "    top1_config = json.load(f)\n",
        "\n",
        "hidden_sizes  = top1_config.get(\"hidden_layer_sizes\")\n",
        "dropout  = top1_config.get(\"dropout\")\n",
        "activation_str  = top1_config.get(\"activation_fn\")\n",
        "activation_fn  = load_activation_fn(activation_str)\n",
        "lr = top1_config.get(\"learning_rate\")\n",
        "\n",
        "input_triplet = 4 # concate x1, x3 = [T1, nb1, T3, nb3]\n",
        "output_triplet = 2 # P, S\n",
        "\n",
        "# Instantiate the model, without  previous weights just hiperparameters !!!\n",
        "triplet_model = Interpolation(in_size = input_triplet,\n",
        "                              out_size = output_triplet,\n",
        "                              hidden_layer_sizes=hidden_sizes,\n",
        "                              dropout=dropout,\n",
        "                              activation=activation_fn).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(triplet_model.parameters(), lr)"
      ],
      "metadata": {
        "id": "PAi4h9efkG6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_triplet_model(model, in_train, out_train, in_val, out_val,\n",
        "                        criterion, optimizer, device, epochs, save_path):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds_train = model(in_train.to(device))\n",
        "        loss_train = criterion(preds_train, out_train.to(device))\n",
        "\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds_val = model(in_val.to(device))\n",
        "            loss_val = criterion(preds_val, out_val.to(device))\n",
        "\n",
        "        train_losses.append(loss_train.item())\n",
        "        val_losses.append(loss_val.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1:03d} | Train Loss: {loss_train.item():.5e} | Val Loss: {loss_val.item():.5e}\")\n",
        "\n",
        "        if loss_val.item() < best_val_loss:\n",
        "            best_val_loss = loss_val.item()\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"  Best model saved (val_loss = {best_val_loss:.5e})\")\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "f0GFdcuDqN8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path  = os.path.join(output_dir, \"best_triplet_model.pth\")\n",
        "\n",
        "train_losses, val_losses = train_triplet_model(\n",
        "    model=triplet_model,\n",
        "    in_train=in_train_triplets,\n",
        "    out_train=out_train_targets,\n",
        "    in_val=in_val_triplets,\n",
        "    out_val=out_val_targets,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    epochs=150,\n",
        "    save_path=save_path\n",
        ")\n",
        "\n",
        "losses_df = pd.DataFrame({\n",
        "    \"epoch\": list(range(1, len(train_losses) + 1)),\n",
        "    \"train_loss\": train_losses,\n",
        "    \"val_loss\": val_losses\n",
        "})\n",
        "\n",
        "losses_csv_path = os.path.join(output_dir, \"triplet_model_losses.csv\")\n",
        "losses_df.to_csv(losses_csv_path, index=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses_df[\"epoch\"], losses_df[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
        "plt.plot(losses_df[\"epoch\"], losses_df[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.title(\"TripletNet Training and Validation Loss\")\n",
        "plt.yscale(\"log\")\n",
        "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, \"triplet_model_loss_curve.png\"), dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QqcZIebqmcBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN and B-SPlines INTERPOLATION"
      ],
      "metadata": {
        "id": "dENyCvbnj4xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #######################################################\n",
        "# INTERPOLATION FOR THE TRIPLET\n",
        "# #######################################################\n",
        "def Interpolation_Triplet_test(model, inputs_set, outputs_set, scaler_P, scaler_S, device, indices, save_dir, gap):\n",
        "    model.eval()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    metrics = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for i, center_index in enumerate(indices):\n",
        "        start_time = time.time()\n",
        "        idx1, idx2, idx3 = get_interpolation_indices(center_index, len(inputs_set), gap)\n",
        "\n",
        "        if idx1 is None:\n",
        "            print(f\"Saltando índice {center_index} (gap={gap}) debido a límites.\")\n",
        "            continue\n",
        "\n",
        "        x1 = inputs_set[idx1]\n",
        "        x3 = inputs_set[idx3]\n",
        "\n",
        "        y1 = outputs_set[idx1]\n",
        "        y2_true = outputs_set[idx2]\n",
        "        y3 = outputs_set[idx3]\n",
        "\n",
        "        # Concatenate x1 y x3 for input in the Interpolation NN\n",
        "        triplet_input_tensor = torch.cat([\n",
        "            torch.FloatTensor(x1).unsqueeze(0),\n",
        "            torch.FloatTensor(x3).unsqueeze(0)\n",
        "        ], dim=1).to(device)\n",
        "\n",
        "        # Real outputs\n",
        "        P_true = 10 ** np.array([y1[0], y2_true[0], y3[0]])\n",
        "        S_true = 10 ** np.array([y1[1], y2_true[1], y3[1]])\n",
        "\n",
        "        # Predictions\n",
        "        with torch.no_grad():\n",
        "            y2_pred_scaled = model(triplet_input_tensor).cpu().numpy()[0]\n",
        "\n",
        "        P2_pred = inv_log_scaled(y2_pred_scaled[0], scaler_P)\n",
        "        S2_pred = inv_log_scaled(y2_pred_scaled[1], scaler_S)\n",
        "\n",
        "        # Metrics\n",
        "        mse_P = mean_squared_error([P_true[1]], [P2_pred])\n",
        "        mae_P = mean_absolute_error([P_true[1]], [P2_pred])\n",
        "        mse_S = mean_squared_error([S_true[1]], [S2_pred])\n",
        "        mae_S = mean_absolute_error([S_true[1]], [S2_pred])\n",
        "\n",
        "        metrics.append({\n",
        "            \"method\": \"TripletNet\", \"index\": int(center_index),\n",
        "            \"gap\": gap,\n",
        "            \"mse_P\": mse_P, \"mae_P\": mae_P,\n",
        "            \"mse_S\": mse_S, \"mae_S\": mae_S\n",
        "        })\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        for m in metrics:\n",
        "            m[\"elapsed_sec\"] = elapsed\n",
        "\n",
        "\n",
        "        # Interpolation graph\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        x_labels = [f'Idx {idx1}', f'Idx {idx2}', f'Idx {idx3}']\n",
        "        x_ticks = np.arange(3)\n",
        "\n",
        "        axs[0].plot(x_ticks, P_true, label='True P', marker='o', color='green')\n",
        "        axs[0].scatter(x_ticks[1], P2_pred, label='Predicted P', marker='x', s=100, color='red')\n",
        "        axs[0].set_title('Pressure (P) [MeV/fm³]', fontsize=14)\n",
        "        axs[0].set_xticks(x_ticks)\n",
        "        axs[0].set_xticklabels(x_labels, fontsize=14)\n",
        "        axs[0].tick_params(axis='y', labelsize=14)\n",
        "        axs[0].grid(True)\n",
        "        axs[0].legend(fontsize=14)\n",
        "        axs[0].ticklabel_format(axis='y', style='sci', scilimits=(-3, 3))\n",
        "\n",
        "        axs[1].plot(x_ticks, S_true, label='True S', marker='o', color='blue')\n",
        "        axs[1].scatter(x_ticks[1], S2_pred, label='Predicted S', marker='x', s=100, color='orange')\n",
        "        axs[1].set_title('Entropy (S) [1/fm³]', fontsize=14)\n",
        "        axs[1].set_xticks(x_ticks)\n",
        "        axs[1].tick_params(axis='y', labelsize=14)\n",
        "        axs[1].set_xticklabels(x_labels, fontsize=14)\n",
        "        axs[1].grid(True)\n",
        "        axs[1].legend(fontsize=14)\n",
        "        axs[1].ticklabel_format(axis='y', style='sci', scilimits=(-3, 3))\n",
        "\n",
        "        plt.suptitle(f\"NN Local Interpolation Test #{i+1} (Index {center_index})  (Gap {gap})\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        filename = f\"TripletNet_triplet_{i+1}_index_{center_index}.png\"\n",
        "        plt.savefig(os.path.join(save_dir, filename), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "FLKUYggLd6Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "# b-SPLINES INTERPOLATION\n",
        "#######################################################\n",
        "def interpolation_test_bspline(in_train, out_train, in_test, out_test, indices, save_dir, gap):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    x = in_train[:, 0]\n",
        "    y = in_train[:, 1]\n",
        "    z_P = out_train[:, 0]\n",
        "    z_S = out_train[:, 1]\n",
        "\n",
        "    bspline_P = SmoothBivariateSpline(x, y, z_P, kx=3, ky=3)\n",
        "    bspline_S = SmoothBivariateSpline(x, y, z_S, kx=3, ky=3)\n",
        "\n",
        "    metrics = []\n",
        "\n",
        "    for i, center_index in enumerate(indices):\n",
        "        if center_index < 1 or center_index >= len(in_test) - 1:\n",
        "            continue\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        x1 = in_test[center_index - gap]\n",
        "        x2 = in_test[center_index]\n",
        "        x3 = in_test[center_index + gap]\n",
        "\n",
        "        y1 = out_test[center_index - gap]\n",
        "        y2_true = out_test[center_index]\n",
        "        y3 = out_test[center_index + gap]\n",
        "\n",
        "        # Predicions for middle point\n",
        "        P_pred_log = bspline_P.ev(x2[0], x2[1])\n",
        "        S_pred_log = bspline_S.ev(x2[0], x2[1])\n",
        "\n",
        "        # Inverse log transformation to get real phyisical values\n",
        "        P_true = 10 ** np.array([y1[0], y2_true[0], y3[0]])\n",
        "        S_true = 10 ** np.array([y1[1], y2_true[1], y3[1]])\n",
        "        P_pred = 10 ** P_pred_log\n",
        "        S_pred = 10 ** S_pred_log\n",
        "\n",
        "        # Metrics\n",
        "        mse_P = mean_squared_error([P_true[1]], [P_pred])\n",
        "        mae_P = mean_absolute_error([P_true[1]], [P_pred])\n",
        "        mse_S = mean_squared_error([S_true[1]], [S_pred])\n",
        "        mae_S = mean_absolute_error([S_true[1]], [S_pred])\n",
        "\n",
        "        metrics.append({\n",
        "            \"method\": \"B-Spline\",\n",
        "            \"index\": int(center_index),\n",
        "            \"mse_P\": mse_P,\n",
        "            \"mae_P\": mae_P,\n",
        "            \"mse_S\": mse_S,\n",
        "            \"mae_S\": mae_S\n",
        "        })\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        for m in metrics:\n",
        "            m[\"elapsed_sec\"] = elapsed\n",
        "\n",
        "        # Plot\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        x_labels = [f'Idx {center_index-gap}', f'Idx {center_index}', f'Idx {center_index+gap}']\n",
        "        x_ticks = np.arange(3)\n",
        "\n",
        "        axs[0].plot(x_ticks, P_true, label='True P', marker='o', color='green')\n",
        "        axs[0].scatter(x_ticks[1], P_pred, label='Predicted P', marker='x', s=100, color='red')\n",
        "        axs[0].set_title('Pressure (P) [MeV/fm³]', fontsize=14)\n",
        "        axs[0].set_xticks(x_ticks)\n",
        "        axs[0].set_xticklabels(x_labels, fontsize=14)\n",
        "        axs[0].tick_params(axis='y', labelsize=14)\n",
        "        axs[0].grid(True)\n",
        "        axs[0].legend(fontsize=14)\n",
        "        axs[0].ticklabel_format(axis='y', style='sci', scilimits=(-3, 3))\n",
        "\n",
        "        axs[1].plot(x_ticks, S_true, label='True S', marker='o', color='blue')\n",
        "        axs[1].scatter(x_ticks[1], S_pred, label='Predicted S', marker='x', s=100, color='orange')\n",
        "        axs[1].set_title('Entropy (S) [1/fm³]', fontsize=14)\n",
        "        axs[1].set_xticks(x_ticks)\n",
        "        axs[1].tick_params(axis='y', labelsize=14)\n",
        "        axs[1].set_xticklabels(x_labels, fontsize=14)\n",
        "        axs[1].grid(True)\n",
        "        axs[1].legend(fontsize=14)\n",
        "        axs[1].ticklabel_format(axis='y', style='sci', scilimits=(-3, 3))\n",
        "\n",
        "        plt.suptitle(f\"B-Spline Local Triplet Test #{i+1} (Index {center_index})\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        filename = f\"SPLINE_triplet_{i+1}_index_{center_index}.png\"\n",
        "        plt.savefig(os.path.join(save_dir, filename), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "YG6qU9XuvcQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload top1 model hyperparameteres\n",
        "output_dir  = \"/content/drive/My Drive/Colab Notebooks/Random_16_06/Triplet\"\n",
        "best_model_path = os.path.join(output_dir , \"best_triplet_model.pth\")\n",
        "\n",
        "triplet_model.load_state_dict(torch.load(best_model_path))\n",
        "triplet_model.eval()\n",
        "\n",
        "# Define indices\n",
        "print(f\"Indices for InterpolationTripletNe from test data set: {indices}\")\n",
        "\n",
        "# Define saving directory\n",
        "triplet_nn_save_base_dir = output_dir\n",
        "os.makedirs(triplet_nn_save_base_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "0v9r8v3M0Jcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for current_gap in [1, 5]:\n",
        "    print(f\"\\n▶️ TripletNet and B-Spline with gap={current_gap}...\")\n",
        "\n",
        "    save_dir_gap = os.path.join(triplet_nn_save_base_dir, f\"gap_{current_gap}\")\n",
        "    os.makedirs(save_dir_gap, exist_ok=True)\n",
        "\n",
        "    #### TripletNet ####\n",
        "    metrics_triplet_nn_gap = Interpolation_Triplet_test(\n",
        "        model=triplet_model,\n",
        "        inputs_set=in_test,\n",
        "        outputs_set=out_test,\n",
        "        scaler_P=scaler_P,\n",
        "        scaler_S=scaler_S,\n",
        "        device=device,\n",
        "        indices=indices,\n",
        "        save_dir=save_dir_gap,\n",
        "        gap=current_gap\n",
        "    )\n",
        "    df_nn = pd.DataFrame(metrics_triplet_nn_gap)\n",
        "\n",
        "    #### B-Spline ####\n",
        "    metrics_spline_gap = interpolation_test_bspline(\n",
        "        in_train=in_train,\n",
        "        out_train=out_train,\n",
        "        in_test=in_test,\n",
        "        out_test=out_test,\n",
        "        indices=indices,\n",
        "        save_dir=save_dir_gap,\n",
        "        gap=current_gap\n",
        "    )\n",
        "\n",
        "    df_spline = pd.DataFrame(metrics_spline_gap)\n",
        "\n",
        "    df_all = pd.concat([df_nn, df_spline], ignore_index=True)\n",
        "    csv_path = os.path.join(save_dir_gap, f\"metrics_triplet_gap_{current_gap}.csv\")\n",
        "    df_all.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "id": "QOb4hUNi7n71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}