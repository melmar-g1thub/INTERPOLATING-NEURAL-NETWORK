{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcRSF41nj0eecaXqyfx9dV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melmar-g1thub/INTERPOLATING-NEURAL-NETWORK/blob/main/preparation_of_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-w8OGEIEjpa"
      },
      "source": [
        "PREPOCESSING OF DATA\n",
        "- Upload all the necesary libraries used throughout the modules\n",
        "- Upload the data files and create the indexed mapping\n",
        "- Preparte data for NN training:\n",
        "1. Splitting into train/validation/test sets for finding parameters, hyperparameters and generalization\n",
        "2. Normalization\n",
        "3. Convert into nn.module Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmbviN_LEDIl"
      },
      "source": [
        "### UPLOADING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnwEDg8g6Yli"
      },
      "outputs": [],
      "source": [
        "!pip install skorch\n",
        "from skorch import NeuralNetRegressor\n",
        "from skorch.callbacks import EarlyStopping\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import seaborn as sns\n",
        "import random\n",
        "import itertools\n",
        "import json\n",
        "import joblib\n",
        "import time\n",
        "import ast\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, QuantileTransformer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import randint, loguniform\n",
        "from scipy.interpolate import griddata, SmoothBivariateSpline\n",
        "\n",
        "try:\n",
        "    from torch.amp import autocast\n",
        "except ImportError:\n",
        "    from torch.cuda.amp import autocast\n",
        "\n",
        "try:\n",
        "    from torch.amp import GradScaler\n",
        "except ImportError:\n",
        "    from torch.cuda.amp import GradScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UPLOAD DATA"
      ],
      "metadata": {
        "id": "jDY5-fpooRSy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvz3RlXG6a4o"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "import os\n",
        "if not os.path.exists(\"/content/neutron-star-eos\"):\n",
        "    !git clone https://github.com/tu-usuario/neutron-star-eos.git\n",
        "    data_path = \"/content/neutron-star-eos/data/\"\n",
        "\n",
        "# temperature and baryon number data\n",
        "eos_t = np.loadtxt(data_path + 'eos.t', skiprows=2)  # 81 values\n",
        "eos_nb = np.loadtxt(data_path + 'eos.nb', skiprows=2)  # 326 values\n",
        "eos_yq = np.zeros(len(eos_t))  # Constant value of 0\n",
        "\n",
        "# thermodynamic data\n",
        "thermo_data = np.loadtxt(data_path + 'eos.thermo', skiprows=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0oZMlds8ySZ"
      },
      "outputs": [],
      "source": [
        "# Extract indices and thermodynamic properties\n",
        "indices = thermo_data[:, :3].astype(int)  # i_T, i_nb, i_Yq\n",
        "q_values = thermo_data[:, 3:5]  # Q1 (pressure) and Q2 (entropy)\n",
        "\n",
        "# Convert to zero-based index\n",
        "i_t = indices[:, 0] - 1\n",
        "i_nb = indices[:, 1] - 1\n",
        "\n",
        "# Ensure indices are valid, np.where() to get valid integer and not boolean\n",
        "valid_indices = np.where((i_t >= 0) & (i_t < len(eos_t)) & (i_nb >= 0) & (i_nb < len(eos_nb)))[0]\n",
        "T_valid = eos_t[i_t[valid_indices]]\n",
        "nb_valid = eos_nb[i_nb[valid_indices]]\n",
        "q_valid = q_values[valid_indices]\n",
        "\n",
        "logT = np.log10(T_valid)\n",
        "log_nb = np.log10(nb_valid)\n",
        "\n",
        "P_valid = q_valid[:, 0] * nb_valid\n",
        "S_valid = q_valid[:, 1] * nb_valid\n",
        "\n",
        "logP = np.log10(np.clip(P_valid, 1e-10, None))  # protects invalid log10(0)\n",
        "logS = np.log10(np.clip(S_valid, 1e-10, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMVe24dVD_qx"
      },
      "outputs": [],
      "source": [
        "# Filter central 90%\n",
        "T_q05, T_q95 = np.quantile(logT, [0.05, 0.95])\n",
        "nb_q05, nb_q95 = np.quantile(log_nb, [0.05, 0.95])\n",
        "mask = (logT >= T_q05) & (logT <= T_q95) & \\\n",
        "       (log_nb >= nb_q05) & (log_nb <= nb_q95)\n",
        "\n",
        "T_values = logT[mask]\n",
        "nb_values = log_nb[mask]\n",
        "P_values = logP[mask]\n",
        "S_values = logS[mask]\n",
        "\n",
        "inputs = np.vstack([T_values, nb_values]).T\n",
        "outputs = np.vstack([P_values, S_values]).T\n",
        "\n",
        "print(\"Initial inputs shape (before splitting/scaling):\", inputs.shape)\n",
        "print(\"Initial outputs shape (before splitting/scaling):\", outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH3C6V5s481H"
      },
      "outputs": [],
      "source": [
        "# Histograms for individual features\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.histplot(inputs[:, 0], kde=True, bins=50, color='skyblue')\n",
        "plt.title('Distribution of log10(Temperature)', fontsize=18)\n",
        "plt.xlabel('log10(T)', fontsize=17)\n",
        "plt.ylabel('Count', fontsize=17)\n",
        "plt.tick_params(axis='both', labelsize=14)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.histplot(inputs[:, 1], kde=True, bins=50, color='lightcoral')\n",
        "plt.title('Distribution of log10(Baryon Number)', fontsize=18)\n",
        "plt.xlabel('log10(nb)', fontsize=17)\n",
        "plt.ylabel('Count', fontsize=17)\n",
        "plt.tick_params(axis='both', labelsize=14)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.histplot(outputs[:, 0], kde=True, bins=50, color='lightgreen')\n",
        "plt.title('Distribution of log10(Pressure)', fontsize=18)\n",
        "plt.xlabel('log10(P)', fontsize=17)\n",
        "plt.ylabel('Count', fontsize=17)\n",
        "plt.tick_params(axis='both', labelsize=14)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.histplot(outputs[:, 1], kde=True, bins=50, color='orchid')\n",
        "plt.title('Distribution of log10(Entropy)', fontsize=18)\n",
        "plt.xlabel('log10(S)', fontsize=17)\n",
        "plt.ylabel('Count', fontsize=17)\n",
        "plt.tick_params(axis='both', labelsize=14)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5, hspace=0.8)\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Distributions of Log-Transformed EoS Features (After 90% Filter)', y=1.02, fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjk3NtU86J7f"
      },
      "outputs": [],
      "source": [
        "# 2D Histogram / Density Plot for Input Parameters (T vs nb)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.hist2d(inputs[:, 0], inputs[:, 1], bins=50, cmap='viridis')\n",
        "plt.colorbar(label='Count')\n",
        "plt.xlabel('log10(T)')\n",
        "plt.ylabel('log10(nb)')\n",
        "plt.title('2D Distribution of Input Parameters (T vs nb)')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.savefig(os.path.join(plot_save_path, '2d_input_distribution.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.kdeplot(x=inputs[:, 0], y=inputs[:, 1], fill=True, cmap='viridis', levels=20)\n",
        "plt.xlabel('log10(T)')\n",
        "plt.ylabel('log10(nb)')\n",
        "plt.title('2D Density Plot of Input Parameters (T vs nb)')\n",
        "plt.savefig(os.path.join(plot_save_path, '2d_density_plot.png'), dpi=300, bbox_inches='tight')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWwpA8rzEv7Z"
      },
      "source": [
        "From the histograms we check:\n",
        "- Inputs (T and nb) are uniformly distributed, showing approximatly the same number of counts across their range\n",
        "- Outputs are slightly skewed, S specifically shows a less gaussian behaviour\n",
        "\n",
        "Stratification of inputs will not be required as after splitting we'll still get pretty uniform distributions for each set.\n",
        "We will look carefully into how splitting and normalization affect to the outputs of the 3 data sets, as we hope them all to be representative of the behaviour of the EoS.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Juq6pWTCEF-G"
      },
      "source": [
        "### PREPARATION OF DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbbVZf7mNQUr"
      },
      "source": [
        "Normalization of the files needs to be performed AFTER it's been divided in the train-validation-test sets!!!!!!!\n",
        "Because if we perform it before it causes:\n",
        "- Global Statistics: normalization process calculates these statistics (mean, std, min, max) using all the data points, including those in what will eventually become your test set.\n",
        "- Unfair Advantage: training set's scaled values are influenced by the statistics of the test set. This means your model indirectly \"sees\" information about the test data during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNqsYRobwyv9"
      },
      "outputs": [],
      "source": [
        "# 1. Split the data\n",
        "# First split: 80% train, 20% temp (for validation/test)\n",
        "in_train, in_temp, out_train, out_temp = train_test_split(\n",
        "    inputs, outputs, test_size=0.20, random_state=42, shuffle=True)\n",
        "\n",
        "# Second split: From the 20% temp, split it into 15% validation and 5% test\n",
        "in_val, in_test, out_val, out_test = train_test_split(\n",
        "    in_temp, out_temp, test_size=0.25, random_state=42, shuffle=True)\n",
        "\n",
        "print(f\"Train samples: {len(in_train)}\")\n",
        "print(f\"Validation samples: {len(in_val)}\")\n",
        "print(f\"Test samples: {len(in_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_GdiW4nN3Sj"
      },
      "source": [
        "Fit the Scaler ONLY on the Training Data, this calculates the necessary scaling parameters (mean, std, min, max) exclusively from the training distribution.\n",
        "\n",
        "Then, we apply transform() to the test and validation data, using the same fitted scaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zoHqiko96rK"
      },
      "outputs": [],
      "source": [
        "# 2. Normalization\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler_T = StandardScaler()\n",
        "scaler_nb = StandardScaler()\n",
        "scaler_P =  QuantileTransformer(output_distribution='normal', n_quantiles=1000, subsample=1_000_000, random_state=42)\n",
        "scaler_S = QuantileTransformer(output_distribution='normal', n_quantiles=1000, subsample=1_000_000, random_state=42)\n",
        "\n",
        "# Save the scalers in case we later re-study the model\n",
        "scaler_save_path = \"/content/drive/My Drive/Colab Notebooks/TFG_NN_FINAL/SCALERS\"\n",
        "scaler_P = joblib.load(os.path.join(scaler_save_path, \"scaler_P.pkl\"))\n",
        "scaler_S = joblib.load(os.path.join(scaler_save_path, \"scaler_S.pkl\"))\n",
        "scaler_T = joblib.load(os.path.join(scaler_save_path, \"scaler_T.pkl\"))\n",
        "scaler_nb = joblib.load(os.path.join(scaler_save_path, \"scaler_nb.pkl\"))\n",
        "\n",
        "# StandardScaler:\tGaussian-like distribution, improves convergence with activation functions\n",
        "# MinMaxScaler: Best when we want normalized plots or comparisons, may want to clip/clamp outputs easily and care about scaled MAE/MSE\n",
        "# QuantileTransformer: For highly skewed distributions, normalize close to gaussian\n",
        "\n",
        "# We fit the scaler ONLY ON TRAINING SET\n",
        "def fit_train_scaler(tensor, scaler):\n",
        "  return scaler.fit_transform(tensor.reshape(-1, 1))\n",
        "\n",
        "# Then we simply apply the transformation to validation and test set\n",
        "def apply_scaler(tensor, scaler):\n",
        "  return scaler.transform(tensor.reshape(-1, 1))\n",
        "\n",
        "### INPUTS ###\n",
        "in_T_train_scaled = fit_train_scaler(in_train[:, 0], scaler_T)\n",
        "in_nb_train_scaled = fit_train_scaler(in_train[:, 1], scaler_nb)\n",
        "\n",
        "in_T_val_scaled = apply_scaler(in_val[:, 0], scaler_T)\n",
        "in_nb_val_scaled = apply_scaler(in_val[:, 1], scaler_nb)\n",
        "in_T_test_scaled = apply_scaler(in_test[:, 0], scaler_T)\n",
        "in_nb_test_scaled = apply_scaler(in_test[:, 1], scaler_nb)\n",
        "\n",
        "# Recombine inputs for each set\n",
        "in_train_processed = np.hstack((in_T_train_scaled, in_nb_train_scaled))\n",
        "in_val_processed = np.hstack((in_T_val_scaled, in_nb_val_scaled))\n",
        "in_test_processed = np.hstack((in_T_test_scaled, in_nb_test_scaled))\n",
        "\n",
        "\n",
        "### OUTPUTS ###\n",
        "out_P_train_scaled = fit_train_scaler(out_train[:, 0], scaler_P)\n",
        "out_S_train_scaled = fit_train_scaler(out_train[:, 1], scaler_S)\n",
        "\n",
        "out_P_val_scaled = apply_scaler(out_val[:, 0], scaler_P)\n",
        "out_S_val_scaled = apply_scaler(out_val[:, 1], scaler_S)\n",
        "out_P_test_scaled = apply_scaler(out_test[:, 0], scaler_P)\n",
        "out_S_test_scaled = apply_scaler(out_test[:, 1], scaler_S)\n",
        "\n",
        "out_train_processed = np.hstack((out_P_train_scaled, out_S_train_scaled))\n",
        "out_val_processed = np.hstack((out_P_val_scaled, out_S_val_scaled))\n",
        "out_test_processed = np.hstack((out_P_test_scaled, out_S_test_scaled))\n",
        "\n",
        "\n",
        "# Check the shapes of the input and output arrays\n",
        "print(\"Training: inputs shape:\", in_train_processed.shape)  # (number_of_samples, 2)\n",
        "print(\"Training: Outputs shape:\", out_train_processed.shape)\n",
        "\n",
        "# If we were to upload the scalers again\n",
        "# scaler_save_path = \"/content/drive/My Drive/Colab Notebooks/TFG_NN_FINAL/SCALERS\"\n",
        "# joblib.dump(scaler_P, os.path.join(scaler_save_path, \"scaler_P.pkl\"))\n",
        "# joblib.dump(scaler_S, os.path.join(scaler_save_path, \"scaler_S.pkl\"))\n",
        "# joblib.dump(scaler_T, os.path.join(scaler_save_path, \"scaler_T.pkl\"))\n",
        "# joblib.dump(scaler_nb, os.path.join(scaler_save_path, \"scaler_nb.pkl\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc2wfr3D-Rgo"
      },
      "outputs": [],
      "source": [
        "# Visualize normalization for Input Features (T and nb) and Output (logP and logS) features\n",
        "def visualize_normalization(original_input, normalized_input, original_output, normalized_output, dataset_name, save_path):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # INPUTS\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    axs[0, 0].hist(original_input[:, 0], bins=50, alpha=0.7, color=\"orange\")\n",
        "    axs[0, 0].set_title('Original log10(T)')\n",
        "    axs[0, 1].hist(normalized_input[:, 0], bins=50, alpha=0.7, color=\"darkorange\")\n",
        "    axs[0, 1].set_title('Normalized log10(T)')\n",
        "\n",
        "    axs[1, 0].hist(original_input[:, 1], bins=50, alpha=0.7, color=\"steelblue\")\n",
        "    axs[1, 0].set_title('Original log10(nb)')\n",
        "    axs[1, 1].hist(normalized_input[:, 1], bins=50, alpha=0.7, color='darkblue')\n",
        "    axs[1, 1].set_title('Normalized log10(nb)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f'Input Data: Original vs. Normalized ({dataset_name} Set)', y=1.02, fontsize=16)\n",
        "    plt.savefig(os.path.join(save_path, f'normalization_input_{dataset_name.lower()}.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # OUTPUTS\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    axs[0, 0].hist(original_output[:, 0], bins=50, alpha=0.7, color=\"green\")\n",
        "    axs[0, 0].set_title('Original log10(P)')\n",
        "    axs[0, 1].hist(normalized_output[:, 0], bins=50, alpha=0.7, color=\"darkgreen\")\n",
        "    axs[0, 1].set_title('Normalized log10(P)')\n",
        "\n",
        "    axs[1, 0].hist(original_output[:, 1], bins=50, alpha=0.7, color=\"red\")\n",
        "    axs[1, 0].set_title('Original log10(S)')\n",
        "    axs[1, 1].hist(normalized_output[:, 1], bins=50, alpha=0.7, color=\"darkred\")\n",
        "    axs[1, 1].set_title('Normalized log10(S)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f'Output Data: Original vs. Normalized ({dataset_name} Set)', y=1.02, fontsize=16)\n",
        "    plt.savefig(os.path.join(save_path, f'normalization_output_{dataset_name.lower()}.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKewoLDyRZWy"
      },
      "outputs": [],
      "source": [
        "# Visualizing the normalization for TRAINING\n",
        "visualize_normalization(\n",
        "    in_train, in_train_processed,\n",
        "    out_train, out_train_processed,\n",
        "    dataset_name=\"Train\",\n",
        "    save_path=plot_save_path\n",
        ")\n",
        "\n",
        "# Visualizing the normalization for VALIDATION\n",
        "visualize_normalization(\n",
        "    in_test, in_val_processed,\n",
        "    out_test, out_val_processed,\n",
        "    dataset_name=\"Validation\",\n",
        "    save_path=plot_save_path\n",
        ")\n",
        "\n",
        "visualize_normalization(\n",
        "    in_test, in_test_processed,\n",
        "    out_test, out_test,\n",
        "    dataset_name=\"Test\",\n",
        "    save_path=plot_save_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3cheUZe67n7"
      },
      "outputs": [],
      "source": [
        "# 3. Convert data to PyTorch tensors\n",
        "in_train_tensor = torch.FloatTensor(in_train_processed)\n",
        "out_train_tensor = torch.FloatTensor(out_train_processed)\n",
        "in_test_tensor = torch.FloatTensor(in_test_processed)\n",
        "out_test_tensor = torch.FloatTensor(out_test_processed)\n",
        "in_val_tensor = torch.FloatTensor(in_val_processed)\n",
        "out_val_tensor = torch.FloatTensor(out_val_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axkkjGjdFd8T"
      },
      "source": [
        "It's clear that after the processing each data set is still representative of the EoS, keeping\n",
        "- the characteristic peaks of q1 and q2 distributions\n",
        "- long right tail in q1\n",
        "- skewness in q2\n",
        "- uniformity in T and nb\n",
        "\n",
        "Moreover we see that the number of counts in each set corresponds to the amount of data that the represent from the total files. Stratification is not required"
      ]
    }
  ]
}