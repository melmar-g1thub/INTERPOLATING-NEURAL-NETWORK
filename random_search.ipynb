{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UP7hdEKMwABJ",
        "0H6K5b2h-ztb"
      ],
      "authorship_tag": "ABX9TyMCuPBFzTzMMpkJ/22wqKet",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melmar-g1thub/INTERPOLATING-NEURAL-NETWORK/blob/main/random_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RANDOM SEARCH TO FIND OPTIMAL HYPER-PARAMETERS\n",
        "\n",
        "The main challenge lies in bridging scikit-learn's RandomizedSearchCV with PyTorch nn.Module.\n",
        "- RandomizedSearchCV expects a scikit-learn estimator, which is a class that implements fit(), predict(), and potentially score().\n",
        "- Interpolation(nn.Module) is a PyTorch model, not directly a scikit-learn estimator.\n",
        "\n",
        "We'll do this with ` skorch ` where `NeuralNetRegressor`provides its own trainig loop. It also implements early stopping."
      ],
      "metadata": {
        "id": "oOW2afVmswa1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP7hdEKMwABJ"
      },
      "source": [
        "#### RANDOM SEARCH - Scikit Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t78FDRghFdBx"
      },
      "source": [
        "We'll be logging the best parameters and model in order to:\n",
        "- Compare all runs\n",
        "- Track randomness/reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h1JZ9L93HWN"
      },
      "outputs": [],
      "source": [
        "# Define the Search Space for the Randomly-searched parameters\n",
        "param_distributions = {\n",
        "    'module__hidden_layer_sizes': [(256, 128), (128, 64), (128, 64, 32), (64, 128, 64), (64, 64, 64)],\n",
        "    'module__activation': [nn.ReLU, nn.LeakyReLU, nn.ELU, nn.SiLU],\n",
        "    'module__dropout': [0.1, 0.2, 0.4, 0.5],\n",
        "    'lr': np.logspace(-4, -2, 20),\n",
        "    'batch_size': [32, 64, 128, 256],\n",
        "}\n",
        "\n",
        "# Defined hyperparameters\n",
        "input_size = 2  # baryon number and temperature\n",
        "output_size = 2 # Q1 and Q2 in eos.thermo\n",
        "\n",
        "n_iterations = 40\n",
        "epochs = 200\n",
        "patience = 25\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Aditional metrics\n",
        "def prediction_variance(y_true, y_pred):\n",
        "    return np.var(y_true,y_pred)\n",
        "\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "var_scorer = make_scorer(prediction_variance, greater_is_better=True)\n",
        "\n",
        "# Define the model\n",
        "# Skorch wrapper\n",
        "model_estimator = NeuralNetRegressor(\n",
        "    module=Interpolation,\n",
        "    module__in_size=input_size,\n",
        "    module__out_size=output_size,\n",
        "    max_epochs=epochs,\n",
        "    lr=0.01,  # Will be overridden during random search\n",
        "    batch_size=64, #Likewise\n",
        "    optimizer=torch.optim.Adam,\n",
        "    criterion=nn.MSELoss,\n",
        "    callbacks=[EarlyStopping(monitor='valid_loss', patience=patience)],\n",
        ")\n",
        "\n",
        "# Define the CrossValidation\n",
        "cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPhhbFj0AqSy"
      },
      "source": [
        "RandomSearchCV implements Cross Validations KFold, RepeatedKFold is recommended for regression tasks\n",
        "\n",
        "It also includes a scoring, the metric must be maximizing: better models result in larger scores\n",
        "For regression, a negative error measure (‘neg_mean_absolute_error‘) makes values closer to zero to represent less prediction error by the model.\n",
        "\n",
        "Once defined, the search is performed by calling the fit() function and providing a dataset used to train and evaluate model hyperparameter combinations using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9_sJ4OuEH2L"
      },
      "outputs": [],
      "source": [
        "def save_random_search_results(search_obj, name_prefix=\"logP_logS_search\"):\n",
        "    base_dir = \"/content/drive/My Drive/Colab Notebooks/09.06/Random\"\n",
        "    exp_dir = os.path.join(base_dir, f\"{name_prefix}\")\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    # CSV + JSON\n",
        "    results_df = pd.DataFrame(search_obj.cv_results_)\n",
        "    results_df.to_csv(os.path.join(exp_dir, \"results_full.csv\"), index=False)\n",
        "\n",
        "    def safe_serialize(obj):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: safe_serialize(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [safe_serialize(v) for v in obj]\n",
        "        elif isinstance(obj, tuple):\n",
        "            return tuple(safe_serialize(v) for v in obj)\n",
        "        elif isinstance(obj, (int, float, str, bool)) or obj is None:\n",
        "            return obj\n",
        "        else:\n",
        "            return str(obj)  # covert to string\n",
        "\n",
        "    with open(os.path.join(exp_dir, \"results_full.json\"), \"w\") as f:\n",
        "        json.dump(safe_serialize(search_obj.cv_results_), f, indent=2)\n",
        "\n",
        "    joblib.dump(search_obj.best_estimator_, os.path.join(exp_dir, \"best_model.pkl\"))\n",
        "    with open(os.path.join(exp_dir, \"best_params.json\"), \"w\") as f:\n",
        "        json.dump({k: str(v) for k, v in search_obj.best_params_.items()}, f, indent=2)\n",
        "\n",
        "    print(f\"\\n Saved results at: {exp_dir}\")\n",
        "    return exp_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "afNMYqjxwG5o"
      },
      "outputs": [],
      "source": [
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model_estimator,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=n_iterations,\n",
        "    cv=cv,\n",
        "    scoring={\n",
        "        'mean_mse': 'neg_mean_squared_error',\n",
        "        'r2': r2_scorer,\n",
        "        'var_pred': var_scorer\n",
        "    },\n",
        "    refit='r2',  # R² as model score\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Convert combined train+val data to PyTorch tensors for the search (NumPy for RandomizedSearchCV)\n",
        "x_train_val = np.concatenate((in_train_processed, in_val_processed), axis=0).astype(np.float32)\n",
        "y_train_val = np.concatenate((out_train_processed, out_val_processed), axis=0).astype(np.float32)\n",
        "\n",
        "start_time = time.time()\n",
        "random_search.fit(x_train_val, y_train_val)\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "minutes, seconds = divmod(elapsed, 60)\n",
        "print(f\"\\n Randomized Search complete.\")\n",
        "print(f\"Total time: {int(minutes)} min {int(seconds)} sec\")\n",
        "\n",
        "save_random_search_results(random_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pf5tZleygyrR"
      },
      "outputs": [],
      "source": [
        "# Log results\n",
        "logfile = '/content/drive/My Drive/Colab Notebooks/09.06/Random/results_full.csv'\n",
        "log_dir = os.path.dirname(logfile)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "results_df = pd.read_csv(logfile)\n",
        "\n",
        "# Clean up and rename columns for clarity\n",
        "columns_to_log = [\n",
        "    'param_module__hidden_layer_sizes',\n",
        "    'param_lr',\n",
        "    'param_module__dropout',\n",
        "    'param_module__activation',\n",
        "    'param_batch_size',\n",
        "    'rank_test_mean_mse',\n",
        "    'rank_test_r2',\n",
        "    'rank_test_var_pred',\n",
        "    'mean_fit_time',\n",
        "    'std_fit_time',\n",
        "    'mean_test_mean_mse',\n",
        "    'mean_test_r2',\n",
        "    'mean_test_var_pred'\n",
        "]\n",
        "\n",
        "df_filtered = results_df[columns_to_log].copy()\n",
        "\n",
        "df_filtered.rename(columns={\n",
        "    'param_module__hidden_layer_sizes': 'hidden_layer_sizes',\n",
        "    'param_lr': 'learning_rate',\n",
        "    'param_module__dropout': 'dropout',\n",
        "    'param_module__activation': 'activation_fn',\n",
        "    'param_batch_size': 'batch_size',\n",
        "    'mean_test_mean_mse': 'mean_neg_mse',\n",
        "    'mean_test_r2': 'mean_r2',\n",
        "    'mean_test_var_pred': 'mean_var'\n",
        "}, inplace=True)\n",
        "\n",
        "df_filtered['mean_mse'] = -df_filtered['mean_neg_mse'] # Convert to positive MSE\n",
        "\n",
        "df_filtered.to_csv(logfile, index=False)\n",
        "print(f\"\\nAll search results saved to: {logfile}\")\n",
        "\n",
        "# Summarize best result\n",
        "print('\\n--- Best Result ---')\n",
        "print('Best cross-validation score (R2): %s' % random_search.best_score_) # Use random_search here\n",
        "print('Best Hyperparameters: %s' % random_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H6K5b2h-ztb"
      },
      "source": [
        "#### EVALUATION OF RANDOM SEARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkxvmOojjN5h"
      },
      "source": [
        "Correlations heatmat gives a direct answer on which hyper-parameters are most influencial. Pairplot visually displays what values or range for each hyper-parameter is responsible for lower MSE (expect better performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilw1n1YzZeBb"
      },
      "outputs": [],
      "source": [
        "# Heatmat of correlations\n",
        "corr_df = df_filtered[['learning_rate', 'dropout', 'batch_size',\n",
        "                         'activation_fn_encoded', 'hidden_layer_encoded', 'mean_mse']]\n",
        "\n",
        "corr_matrix = corr_df.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True, cbar=True)\n",
        "plt.title(\"Heatmap of correlations between hyperparameters and MSE\")\n",
        "plt.tight_layout()\n",
        "#plt.savefig(os.path.join(plot_save_path, 'MSE_heatmap.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFYzW5elfYTR"
      },
      "source": [
        "PAIRPLOT is a multi-plot grid that shows the relationship between each pair of variables in a dataset. It includes:\n",
        "- Scatter plots for every variable vs every other variable.\n",
        "- Distributions for each variable on the diagonal.\n",
        "\n",
        "And it shows:\n",
        "1. Linear or nonlinear relationships (e.g., higher learning rate → higher MSE?)\n",
        "2. Clusters (e.g., some batch sizes perform consistently better?)\n",
        "3. Outliers (points that break patterns)\n",
        "4. Correlations (strong slopes in the scatter plots)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrPAaOwdYxVa"
      },
      "outputs": [],
      "source": [
        "plot_save_path = \"/content/drive/My Drive/Colab Notebooks/09.06/Random/Hyperparameters Analysis\"\n",
        "os.makedirs(plot_save_path, exist_ok=True)\n",
        "\n",
        "# Pairplot for comparing the different combinations of hyperparameters\n",
        "le_act = LabelEncoder()\n",
        "le_layer = LabelEncoder()\n",
        "\n",
        "df_filtered['activation_fn_str'] = df_filtered['activation_fn'].apply(lambda fn: fn if isinstance(fn, str) else fn.__name__)\n",
        "df_filtered['hidden_layer_str'] = df_filtered['hidden_layer_sizes'].astype(str)\n",
        "df_filtered['activation_fn_encoded'] = le_act.fit_transform(df_filtered['activation_fn_str'])\n",
        "df_filtered['hidden_layer_encoded'] = le_layer.fit_transform(df_filtered['hidden_layer_str'])\n",
        "\n",
        "# Set columns for pairplot\n",
        "pairplot_vars = ['learning_rate', 'dropout', 'batch_size', 'mean_mse']\n",
        "\n",
        "# Plot with activation_fn colored\n",
        "g = sns.pairplot(df_filtered, vars=pairplot_vars, hue='activation_fn_str', diag_kind='kde', corner='True')\n",
        "\n",
        "# Adjust legend title and position\n",
        "new_labels = ['ReLU', 'LeakyReLU']\n",
        "for t, l in zip(g._legend.texts, new_labels):\n",
        "    t.set_text(l)\n",
        "g._legend.set_title(\"Activation Function\")\n",
        "g._legend.set_bbox_to_anchor((0.6, 0.8))  # move it outside the plot if needed\n",
        "\n",
        "plt.suptitle(\"Pairplot of Hyperparameters vs MSE\", fontsize=16, y=1.02, x=0.4)\n",
        "plt.savefig(os.path.join(plot_save_path, 'MSE_pairplot_act_fn.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot with hidden_layers colored\n",
        "h = sns.pairplot(df_filtered, vars=pairplot_vars, hue='hidden_layer_str', diag_kind='kde', corner='True')\n",
        "h._legend.set_title(\"Hidden Layers Architecture\")\n",
        "h._legend.set_bbox_to_anchor((0.7, 0.8))\n",
        "plt.suptitle(\"Pairplot of Hyperparameters vs MSE\", fontsize=16, y=1.02)\n",
        "plt.savefig(os.path.join(plot_save_path, 'MSE_pairplot_hidden.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot top best models based on R2, MSE and predictions mean variance\n",
        "\n",
        "logfile = '/content/drive/My Drive/Colab Notebooks/TFG_NN_FINAL/Random_40/results_full.csv'\n",
        "log_dir = os.path.dirname(logfile)\n",
        "os.makedirs(log_dir, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "df = pd.read_csv(logfile)\n",
        "\n",
        "top5_r2 = df.sort_values(by='rank_test_r2').head(5).copy()\n",
        "top5_mse = df.sort_values(by='rank_test_mean_mse').head(5).copy()\n",
        "\n",
        "for df_sub in [top5_r2, top5_mse]:\n",
        "    df_sub['config_label'] = df_sub.apply(\n",
        "        lambda row: f\"{row['hidden_layer_sizes']} | dr={row['dropout']} | batch={row['batch_size']} | lr={row['learning_rate']:.1e}\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# Plot Side-by-side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 8), sharey=True)\n",
        "axs[1].sharey(axs[0])\n",
        "fig.subplots_adjust(wspace=3.5)\n",
        "\n",
        "# R²\n",
        "sns.barplot(x='mean_r2', y='config_label', data=top5_r2, palette='crest', ax=axs[0])\n",
        "axs[0].set_xlabel(\"Mean R² (CV)\", fontsize=19)\n",
        "axs[0].set_ylabel(\"Configurations\", fontsize=19)\n",
        "axs[0].grid(axis='x', linestyle='--', alpha=0.6)\n",
        "axs[0].set_xlim(0.98, 1)\n",
        "axs[0].tick_params(axis='both', labelsize=18)\n",
        "\n",
        "# MSE\n",
        "sns.barplot(x='mean_mse', y='config_label', data=top5_mse, palette='crest', ax=axs[1])\n",
        "axs[1].set_xlabel(\"Mean MSE (CV)\", fontsize=19)\n",
        "axs[1].grid(axis='x', linestyle='--', alpha=0.6)\n",
        "axs[1].set_xlim(0.0005, 0.00125)\n",
        "axs[1].tick_params(axis='y', left=False, labelleft=False)\n",
        "axs[1].set_ylabel(\"\")\n",
        "axs[1].tick_params(axis='both', labelsize=18)\n",
        "formatter = ScalarFormatter(useMathText=True)   # usa notación \"×10ⁿ\" en LaTeX\n",
        "formatter.set_powerlimits((-4, -4))\n",
        "axs[1].xaxis.offsetText.set_fontsize(18)\n",
        "axs[1].xaxis.set_major_formatter(formatter)\n",
        "\n",
        "\n",
        "# Variance (de los top5 por R²)\n",
        "sns.barplot(x='mean_var', y='config_label', data=top5_r2, palette='crest', ax=axs[2])\n",
        "axs[2].set_xlabel(\"Mean Predicted Variance\", fontsize=19)\n",
        "axs[2].grid(axis='x', linestyle='--', alpha=0.6)\n",
        "axs[2].set_xlim(0.95, 1)\n",
        "axs[2].tick_params(axis='y', left=False, labelleft=False)\n",
        "axs[2].set_ylabel(\"\")\n",
        "axs[2].tick_params(axis='both', labelsize=18)\n",
        "\n",
        "plt.tight_layout()\n",
        "output_file = os.path.join(log_dir, \"top5_hyperparameter_comparison.png\")\n",
        "plt.savefig(output_file, dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VcUKUJZjA6Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTbSE7TZbev2"
      },
      "source": [
        "RANDOM FOREST measures how much each input variable (hyperparameter) contributes to reducing the error in the forest:\n",
        "- For each tree in the forest, it checks how much each feature reduces the MSE when it’s used to split a node.\n",
        "- Then it averages these reductions over all trees, giving you one score per hyperparameter.\n",
        "\n",
        "The more it reduces the error, the more important it is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uemvA4X6ZtVf"
      },
      "outputs": [],
      "source": [
        "# SHAP graph\n",
        "# RandomizedSearchCV no entrena un modelo con interpretabilidad directa como un árbol,\n",
        "# usaremos RandomForestRegressor para estimar la importancia relativa de cada hiperparámetro sobre el MSE\n",
        "\n",
        "X_forest = df_filtered[['learning_rate', 'dropout', 'batch_size', 'activation_fn_encoded', 'hidden_layer_encoded']]\n",
        "Y_forest = df_filtered['mean_r2']\n",
        "\n",
        "# Train Forest model to stimate relevance of each parameter\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "rf.fit(X_forest, Y_forest)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "feature_names = X_forest.columns\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=importances, y=feature_names, palette='viridis')\n",
        "plt.title(\"Impact of hyperparameters on R2 (RandomForest estimation)\")\n",
        "plt.xlabel(\"Relative importance\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(plot_save_path, 'R2_randomforest.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ]
}