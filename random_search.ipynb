{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UP7hdEKMwABJ",
        "0H6K5b2h-ztb"
      ],
      "authorship_tag": "ABX9TyNMjWWUMgNXa020LOCWyrXF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RANDOM SEARCH TO FIND OPTIMAL HYPER-PARAMETERS\n",
        "\n",
        "The main challenge lies in bridging scikit-learn's RandomizedSearchCV with PyTorch nn.Module.\n",
        "- RandomizedSearchCV expects a scikit-learn estimator, which is a class that implements fit(), predict(), and potentially score().\n",
        "- Interpolation(nn.Module) is a PyTorch model, not directly a scikit-learn estimator.\n",
        "\n",
        "We'll do this with ` skorch ` where `NeuralNetRegressor`provides its own trainig loop. It also implements early stopping."
      ],
      "metadata": {
        "id": "oOW2afVmswa1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP7hdEKMwABJ"
      },
      "source": [
        "#### RANDOM SEARCH - Scikit Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t78FDRghFdBx"
      },
      "source": [
        "We'll be logging the best parameters and model in order to:\n",
        "- Compare all runs\n",
        "- Track randomness/reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h1JZ9L93HWN"
      },
      "outputs": [],
      "source": [
        "# Define the Search Space for the Randomly-searched parameters\n",
        "# internal modules in skorch require the prefix module__\n",
        "param_distributions = {\n",
        "    'module__hidden_layer_sizes': [(256, 128), (128, 64), (128, 64, 32), (64, 128, 64), (64, 64, 64)],\n",
        "    'module__activation': [nn.ReLU, nn.LeakyReLU, nn.ELU, nn.SiLU],\n",
        "    'module__dropout': [0.1, 0.2, 0.4, 0.5],\n",
        "    'lr': np.logspace(-4, -2, 20),\n",
        "    'batch_size': [32, 64, 128, 256],\n",
        "}\n",
        "\n",
        "# Defined hyperparameters\n",
        "input_size = 2  # baryon number and temperature\n",
        "output_size = 2 # pressure and entropy\n",
        "\n",
        "n_iterations = 40\n",
        "epochs = 200\n",
        "patience = 25\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Aditional metrics\n",
        "def prediction_variance(y_true, y_pred):\n",
        "    return np.var(y_pred)\n",
        "\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "var_scorer = make_scorer(prediction_variance, greater_is_better=True)\n",
        "\n",
        "# Define the model with Skorch wrapper\n",
        "model_estimator = NeuralNetRegressor(\n",
        "    module=Interpolation,\n",
        "    module__in_size=input_size,\n",
        "    module__out_size=output_size,\n",
        "    max_epochs=epochs,\n",
        "    lr=0.01,  # Will be overridden during random search\n",
        "    batch_size=64, #Likewise\n",
        "    optimizer=torch.optim.Adam,\n",
        "    criterion=nn.MSELoss,\n",
        "    callbacks=[EarlyStopping(monitor='valid_loss', patience=patience)],\n",
        ")\n",
        "\n",
        "# Define the CrossValidation\n",
        "cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPhhbFj0AqSy"
      },
      "source": [
        "RandomSearchCV implements Cross Validations KFold, RepeatedKFold is recommended for regression tasks\n",
        "\n",
        "It also includes a scoring, the metric must be maximizing: better models result in larger scores\n",
        "For regression, a negative error measure (‘neg_mean_absolute_error‘) makes values closer to zero to represent less prediction error by the model.\n",
        "\n",
        "Once defined, the search is performed by calling the fit() function and providing a dataset used to train and evaluate model hyperparameter combinations using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9_sJ4OuEH2L"
      },
      "outputs": [],
      "source": [
        "def save_random_search_results(search_obj, name_prefix=\"logP_logS_search\"):\n",
        "    base_dir = # path to save random search results\n",
        "    exp_dir = os.path.join(base_dir, f\"{name_prefix}\")\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    # Create CSV for saving results of random search\n",
        "    results_df = pd.DataFrame(search_obj.cv_results_)\n",
        "    results_df.to_csv(os.path.join(exp_dir, \"results_full.csv\"), index=False)\n",
        "\n",
        "    with open(os.path.join(exp_dir, \"results_full.json\"), \"w\") as f:\n",
        "        json.dump((search_obj.cv_results_), f, indent=2)\n",
        "\n",
        "    print(f\"\\n Saved results at: {exp_dir}\")\n",
        "    return exp_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "afNMYqjxwG5o"
      },
      "outputs": [],
      "source": [
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model_estimator,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=n_iterations,\n",
        "    cv=cv,\n",
        "    scoring={\n",
        "        'mean_mse': 'neg_mean_squared_error',\n",
        "        'r2': r2_scorer,\n",
        "        'var_pred': var_scorer\n",
        "    },\n",
        "    refit='r2',  # R² as model score\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Convert combined train + validation data to NumPy for RandomizedSearchCV\n",
        "x_train_val = np.concatenate((in_train_processed, in_val_processed), axis=0).astype(np.float32)\n",
        "y_train_val = np.concatenate((out_train_processed, out_val_processed), axis=0).astype(np.float32)\n",
        "\n",
        "# Calculate random search and log time\n",
        "start_time = time.time()\n",
        "random_search.fit(x_train_val, y_train_val)\n",
        "end_time = time.time() - start_time\n",
        "print(f\"\\n Randomized Search complete.\")\n",
        "print(f\"Total time: {end_time} sec\")\n",
        "\n",
        "save_random_search_results(random_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pf5tZleygyrR"
      },
      "outputs": [],
      "source": [
        "# Log results\n",
        "logfile = # path where results_full.csv is saved\n",
        "log_dir = os.path.dirname(logfile)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "results_df = pd.read_csv(logfile)\n",
        "\n",
        "# Clean up and rename columns for clarity\n",
        "columns_to_log = [\n",
        "    'param_module__hidden_layer_sizes',\n",
        "    'param_lr',\n",
        "    'param_module__dropout',\n",
        "    'param_module__activation',\n",
        "    'param_batch_size',\n",
        "    'rank_test_mean_mse',\n",
        "    'rank_test_r2',\n",
        "    'rank_test_var_pred',\n",
        "    'mean_fit_time',\n",
        "    'std_fit_time',\n",
        "    'mean_test_mean_mse',\n",
        "    'mean_test_r2',\n",
        "    'mean_test_var_pred'\n",
        "]\n",
        "\n",
        "df_filtered = results_df[columns_to_log].copy()\n",
        "df_filtered.rename(columns={\n",
        "    'param_module__hidden_layer_sizes': 'hidden_layer_sizes',\n",
        "    'param_lr': 'learning_rate',\n",
        "    'param_module__dropout': 'dropout',\n",
        "    'param_module__activation': 'activation_fn',\n",
        "    'param_batch_size': 'batch_size',\n",
        "    'mean_test_mean_mse': 'mean_neg_mse',\n",
        "    'mean_test_r2': 'mean_r2',\n",
        "    'mean_test_var_pred': 'mean_var'\n",
        "}, inplace=True)\n",
        "\n",
        "df_filtered['mean_mse'] = -df_filtered['mean_neg_mse'] # Convert to positive MSE\n",
        "\n",
        "df_filtered.to_csv(logfile, index=False)\n",
        "print(f\"\\nAll search results saved to: {logfile}\")"
      ]
    }
  ]
}